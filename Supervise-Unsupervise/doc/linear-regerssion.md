## Linear Regression

---

### Definition

![alt text](image.png)  
**Reference**: [GeeksforGeeks â€“ Linear Regression](https://www.geeksforgeeks.org/machine-learning/ml-linear-regression/)

**Linear regression** is a supervised learning algorithm that models the relationship between:
- One or more **independent variables** (features, denoted as `X`)
- A **dependent variable** (target, denoted as `y`)  
by fitting a linear equation to the observed data.

---

### How Supervised Learning Works  

1. **Input**: Training set with:  
   - **Features (`x`)**: Input variables (e.g., house size)  
   - **Targets (`y`)**: True output values (e.g., house price)  

2. **Output**: A function **`f`** (the *model*) that:  
   - Takes new input **`x`**  
   - Predicts output **`Å·`** (estimated value)

---

### Key Notation

#### Example: Housing Dataset

| Index | Size (ftÂ²) | Price ($1000's) |
|-------|------------|-----------------|
| 1     | 2104       | 400             |
| 2     | 1416       | 232             |
| 3     | 1534       | 315             |
| 4     | 852        | 178             |
| ...   | ...        | ...             |
| 47    | 3210       | 870             |

#### Variable Definitions

| Symbol     | Meaning                            | Example                |
|------------|------------------------------------|------------------------|
| `x`        | Input feature (independent variable) | House size (2104 sq ft) |
| `y`        | Output target (dependent variable)   | Price ($400,000)        |
| `m`        | Number of training examples          | 47 houses               |
| `(xâ±, yâ±)` | i-th training example                | (2104, 400000)          |
| `Å·`        | Predicted output                     | `Å· = f(x)`              |

---

### Key Characteristics of Linear Regression
![alt text](image-1.png)

1. **Linear Relationship**  
   - Assumes a straight-line relationship between variables

2. **Equation Form**  
   - Simple Linear Regression:  
     ```
     y = wX + b
     ```
     where:  
     - `w` = coefficient (weight)  
     - `b` = y-intercept (bias term)

3. **Objective**  
   - Minimize the difference between predicted values `Å·` and actual values `y` using a **cost function** (e.g., MSE (Mean Squared Error))

4. **Use Cases**  
   - Predicting continuous values (e.g., prices, temperatures)  
   - Understanding feature importance  
   - Trend analysis

---

![Linear Regression Example](../img/Screenshot%202568-06-13%20at%2015.43.50.png)


# ğŸ“‰ Cost Function in Linear Regression

The **cost function** is a mathematical tool used to **quantify the error between predicted outputs and actual target values** in a regression model. It serves as a measure of how well the model's parameters fit the training data.

- **Purpose**: To evaluate the performance of the model by measuring the discrepancy between predicted and actual outcomes.
- **Objective**: Minimize the cost function in order to optimize the model parameters and improve prediction accuracy.





### âœ… Steps to Construct the Cost Function:

1. **Error**:
   \[
   \text{Error}^{(i)} = \hat{y}^{(i)} - y^{(i)}
   \]

2. **Squared Error**:
   \[
   \left( \hat{y}^{(i)} - y^{(i)} \right)^2
   \]

3. **Total Squared Error**:
   \[
   \sum_{i=1}^{m} \left( \hat{y}^{(i)} - y^{(i)} \right)^2
   \]

4. **Average Squared Error**:
   \[
   \frac{1}{m} \sum_{i=1}^{m} \left( \hat{y}^{(i)} - y^{(i)} \right)^2
   \]

5. **Final Cost Function (J)**:
   \[
   J(w, b) = \frac{1}{2m} \sum_{i=1}^{m} \left( f_{w,b}(x^{(i)}) - y^{(i)} \right)^2
   \]

> ğŸ“Œ `J(w, b)` is called the **squared error cost function** and is widely used in linear regression tasks.

> ğŸ’¡ **Why Divide by 2m?**  
> Dividing by 2m is a convention that makes the math easier and more elegant during model training.

---

# Gradient Descent Overview

![alt text](image-3.png)

# Introduction to Gradient Descent

- In previous lessons, we visualized the **cost function** \( J(w, b) \) and how different values of **\( w \)** and **\( b \)** affect it. (look at graph)
- A more systematic method to find the values of \( w \) and \( b \) that minimize the cost is needed â†’ **Gradient Descent**.


#  What is Gradient Descent?

- A general optimization algorithm used across **machine learning**, including:
  - **Linear regression**
  - **Neural networks (deep learning)**
- Can minimize **any cost function**, not just those with two parameters.

# How It Works
![alt text](image-4.png)
1. **Start with initial guesses** for parameters (e.g., \( w = 0 \), \( b = 0 \)).
2. **Iteratively adjust** \( w \) and \( b \) to reduce the cost.
3. Continue updating until reaching a **minimum** value of \( J(w, b) \).

# Visualization Metaphor

- Think of the cost surface \( J(w, b) \) as a **hilly landscape**.
- Gradient descent is like **walking downhill**:
  - Look in all directions.
  - Take a **baby step** in the steepest descent direction.
  - Repeat until reaching a **valley bottom** (local minimum).

# Local Minima Insight

- Some cost functions (e.g., in neural networks) may have **multiple local minima**.
- Gradient descent may end up in **different valleys** depending on the starting point.
- In **linear regression with squared error**, the cost function is **convex**:
  - Always has **one global minimum**.

# Summary

- Gradient descent is a key algorithm in ML for **minimizing cost functions**.
- It works by taking steps in the direction of the **steepest descent**.
- Choice of initial values can affect which **local minimum** you reach.
- Sets the foundation for understanding and implementing learning algorithms.

# Gradient Descent Implementation
![alt text](image-5.png)
## Core Algorithm
- **Parameter Update Rules**:
\[
  w = w - \alpha \frac{\partial}{\partial w} J(w,b)
\]

\[
  b = b - \alpha \frac{\partial}{\partial b} J(w,b)
\]

### **Key Components**
#### **Learning Rate (Î±)**
- Controls step size during optimization.
- Too small (e.g., 0.001): Slow convergence.
- Too large (e.g., 1.0): Risk of overshooting minima.

> Convergence it mean **near local minima**

#### Partial Derivatives

- **âˆ‚J/âˆ‚w**: Gradient with respect to **w**  
- **âˆ‚J/âˆ‚b**: Gradient with respect to **b**

- Interpretation
	- Points in the direction of **steepest ascent**
	- **Negative gradient** â†’ direction of **descent**

### Simultaneous Updates (âœ… Correct Way)

**Update w and b simultaneously by**:
```python
temp_w = w - alpha * derivative_w
temp_b = b - alpha * derivative_b

w = temp_w
b = temp_b
```
- First calculate the updates using old values
- Then apply both updates at the same time


### ğŸš« **Non-Simultaneous Updates (âŒ Incorrect Way)**
In the **incorrect** method:

```python
temp_w = w - alpha * derivative_w
w = temp_w  # â† w updated too early

temp_b = b - alpha * derivative_b  # â† uses already-updated w
b = temp_b

```

**Problem**: The second update uses already-updated w, making it inconsistent. This leads to a different algorithm with different (and usually worse) behavior.

**Visualization**
```
   Start
     â†“
   Compute Gradients
     â†“
Update Parameters â†’ Check Convergence â†’ Stop
     â†‘                             â†“
     â””â”€â”€â”€â”€â”€â”€â”€ Repeat â†â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```



# ğŸ” Gradient Descent â€“ Deeper Intuition
![alt text](image-2.png)

This lesson dives deeper into **how gradient descent works** and why it makes intuitive sense.

  
\[
  w = w - \alpha \frac{\partial}{\partial w} J(w,b)
\]


- **Î± (alpha)**: the **learning rate**, controls the size of the step taken.
- **âˆ‚J/âˆ‚w**: the **derivative (or partial derivative)** of the cost function, indicating the **slope** at a given point.



> - A **derivative(tells you the direction to move)** is the slope of the **tangent line at current w** 
> - The slope helps determine the **direction** and **size** of the parameter update.

---

##  Intuition from Two Scenarios

1. **Starting on the right side of the curve** (slope > 0):
 - Derivative is **positive**
 - `w = w - Î± * (positive)` â†’ w **decreases**
 - You move **left**, toward the **minimum** of the cost function

2. **Starting on the left side of the curve** (slope < 0):
 - Derivative is **negative**
 - `w = w - Î± * (negative)` â†’ w **increases**
 - You move **right**, toward the **minimum**

âœ… In both cases, **gradient descent moves you toward the minimum (that reduce the cost function)**, reducing the cost.

---

## Learning rate



# ğŸ§  Understanding Learning Rate (Î±) in Gradient Descent
![alt text](<Screenshot 2568-06-25 at 15.16.01.png>)

## ğŸ” Gradient Descent Update Rule

\[
w := w - \alpha \cdot \frac{d}{dw}J(w)
\]

- **\( \alpha \)**: Learning rate â€” controls how big each update step is.
- **\( \frac{d}{dw}J(w) \)**: Derivative â€” the slope of the cost function at \( w \).

---

## ğŸš¶â€â™‚ï¸ Case 1: Learning Rate Too Small

- Example: \( \alpha = 0.0000001 \)
- Derivative is multiplied by a tiny number.
- **Very small update steps**.
- âœ… Gradient descent **still converges**, but **very slowly**.
- â³ Requires **many iterations** to reach the minimum.

---

## ğŸƒâ€â™‚ï¸ Case 2: Learning Rate Too Large

- Example: \( \alpha = 1 \) or more.
- Causes **very large jumps**.
- May **overshoot** the minimum and increase cost.
- âŒ Can **diverge** â€” fail to converge to a minimum.

![alt text](<Screenshot 2568-06-25 at 15.22.44.png>)
## ğŸ§˜â€â™‚ï¸ Case 3: Already at a Local Minimum

- At the minimum, the derivative is zero:
  \[
  \frac{d}{dw}J(w) = 0
  \]
- Update becomes:
  \[
  w := w - \alpha \cdot 0 = w
  \]
- âœ… No update is made â€” stays at the minimum.

![alt text](<Screenshot 2568-06-25 at 15.23.00.png>)
## ğŸ“‰ Automatic Step Size Reduction

- Even with fixed \( \alpha \), step size shrinks near the minimum:
  - As \( \frac{d}{dw}J(w) \to 0 \), the update step becomes small.
- This behavior **naturally slows down** convergence near the optimal point.

---

## ğŸ“Š Summary Table

| Learning Rate (Î±)      | Behavior                          | Outcome                         |
|------------------------|------------------------------------|----------------------------------|
| **Too Small (â‰ª 1)**    | Very tiny updates                  | âœ… Converges, but slowly         |
| **Just Right**         | Balanced step size                 | âœ… Efficient convergence         |
| **Too Large (â‰« 1)**    | Overshooting, divergence           | âŒ May never converge            |
| **At Local Minimum**   | Derivative = 0                     | âœ… No change (as desired)        |

---


# ğŸ” Gradient Descent for Linear Regression: Summary

---

## âœ… Goal

To **train a linear regression model** using **gradient descent** with the **squared error cost function**, enabling the model to fit a straight line to the training data.


## ğŸ§® Key Components

### 1. Linear Regression Model

$$
f(w, b, x) = w \cdot x + b
$$

### 2. Squared Error Cost Function

$$
J(w, b) = \frac{1}{2m} \sum_{i=1}^{m} \left(f(w, b, x^{(i)}) - y^{(i)}\right)^2
$$

### 3. Gradient Descent Algorithm
![alt text](image-6.png)

Update rules for minimizing the cost function:

$$
w := w - \alpha \cdot \frac{1}{m} \sum_{i=1}^{m} \left(f(w, b, x^{(i)}) - y^{(i)}\right) \cdot x^{(i)}
$$

$$
b := b - \alpha \cdot \frac{1}{m} \sum_{i=1}^{m} \left(f(w, b, x^{(i)}) - y^{(i)}\right)
$$

Where:
- \( \alpha \) is the **learning rate**
- \( m \) is the number of training examples

---

## ğŸ“ Why These Derivatives?

- Derived using **calculus**, by taking partial derivatives of the cost function with respect to \( w \) and \( b \).
- The \( \frac{1}{2} \) in the cost function helps cancel the \( 2 \) from the derivative of the squared term, simplifying the result.

---

## ğŸ“‰ Convergence and Local Minima


- The **squared error cost function** is a **convex function** (bowl-shaped).
- Convex functions have only **one global minimum** â€” no local minima to get stuck in.
- Therefore, gradient descent is **guaranteed to converge** to the global minimum, **if the learning rate is chosen properly**.

---



# ğŸ§  Comparing Convex and Non-Convex Cost Functions
![alt text](image-7.png)
![alt text](image-8.png)
---

## ğŸŸ¦ Image 1: Squared Error Cost â€“ Convex Function

### âœ… Characteristics:
- **Shape**: Smooth, **bowl-shaped** surface.
- **Function Type**: **Convex**
- **Global Minimum**: Clearly visible at the bottom of the bowl (highlighted).
- **Gradient Descent Behavior**:
  - Always converges to the **global minimum**, regardless of initialization (if learning rate is appropriate).
  - **No local minima** to get stuck in.
- **Use Case**: Common in **linear regression with squared error cost**.

---

## ğŸŸ¥ Image 2: Non-Convex Function â€“ Multiple Local Minima

### âŒ Characteristics:
- **Shape**: Complex surface with **hills and valleys**.
- **Function Type**: **Non-convex**
- **Multiple Minima**: More than one **local minimum** (circled in red).
- **Gradient Descent Behavior**:
  - May get **stuck in a local minimum**, depending on initial values of \( w \) and \( b \).
  - **Not guaranteed** to find the global minimum.
- **Use Case**: Found in **complex models** such as **neural networks** and other non-linear optimization problems.

---

## ğŸ” Summary of the Difference

| Feature                        | **Convex (Image 1)**             | **Non-Convex (Image 2)**         |
|-------------------------------|----------------------------------|----------------------------------|
| Function Type                 | Convex                           | Non-convex                       |
| Number of Minima              | One global minimum               | Multiple local minima            |
| Gradient Descent Convergence | Always to global minimum         | May get stuck in local minimum   |
| Example                       | Linear Regression (Squared Loss) | Neural Networks, Complex Models  |

# ğŸ¬ Gradient Descent in Action â€“ Summary

---
![alt text](image-9.png)

## ğŸ”§ Visual Setup

- **Top Left**: Plot of the linear regression model and training data.
- **Top Right**: Contour plot of the cost function \( J(w, b) \).
- **Bottom**: 3D surface plot of the same cost function.

---



## ğŸªœ Gradient Descent Steps

- Each **step** of gradient descent:
  - Moves the point \( (w, b) \) on the cost surface.
  - Slightly improves the line fit to the data.
- As steps continue:
  - The cost \( J(w, b) \) **decreases**.
  - Parameters follow a **trajectory** toward the **global minimum**.
  - The fitted line becomes more accurate.

---

## ğŸ¯ Final Outcome

- Gradient descent eventually reaches the **global minimum**.
- At the global minimum:
  - The straight line is a **good fit** for the data.
  - Model can now make **predictions**.
    - Example: For a 1250 sq. ft. house, predict â‰ˆ \$250,000.

---

## ğŸ“¦ Batch Gradient Descent
![alt text](image-10.png)
- This version of gradient descent is called **batch gradient descent**.
- Why?
  - Because **every step** uses **all training examples** to compute the gradient.
- Term comes from:
  - Processing the **entire batch** (dataset) during each update.

---


