# ‡∏ó‡∏≤‡∏á‡πÄ‡∏•‡∏∑‡∏≠‡∏Å‡∏Ç‡∏≠‡∏á Gradient Descent: Normal Equation

## üî∑ 1. Normal Equation ‡∏Ñ‡∏∑‡∏≠‡∏≠‡∏∞‡πÑ‡∏£?

Normal Equation ‡∏Ñ‡∏∑‡∏≠‡∏ß‡∏¥‡∏ò‡∏µ‡∏Å‡∏≤‡∏£‡∏´‡∏≤‡∏Ñ‡πà‡∏≤‡∏û‡∏≤‡∏£‡∏≤‡∏°‡∏¥‡πÄ‡∏ï‡∏≠‡∏£‡πå‡∏Ç‡∏≠‡∏á‡πÇ‡∏°‡πÄ‡∏î‡∏• **Linear Regression** ‡πÇ‡∏î‡∏¢‡πÑ‡∏°‡πà‡∏ï‡πâ‡∏≠‡∏á‡πÉ‡∏ä‡πâ‡∏Å‡∏≤‡∏£‡∏ó‡∏≥‡∏ã‡πâ‡∏≥ (iterations) ‡πÅ‡∏ö‡∏ö Gradient Descent

### ‡∏™‡∏°‡∏Å‡∏≤‡∏£‡∏Ç‡∏≠‡∏á Linear Regression:

$$
\hat{y} = Xw
$$

- \( X \): ‡πÄ‡∏°‡∏ó‡∏£‡∏¥‡∏Å‡∏ã‡πå‡∏Ç‡∏≠‡∏á‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏• input (‡∏£‡∏ß‡∏° bias term ‡∏î‡πâ‡∏ß‡∏¢)  
- \( w \): ‡πÄ‡∏ß‡∏Å‡πÄ‡∏ï‡∏≠‡∏£‡πå‡∏Ç‡∏≠‡∏á‡∏û‡∏≤‡∏£‡∏≤‡∏°‡∏¥‡πÄ‡∏ï‡∏≠‡∏£‡πå (‡∏£‡∏ß‡∏° bias term ‡∏î‡πâ‡∏ß‡∏¢)  
- \( \hat{y} \): ‡∏Ñ‡πà‡∏≤‡∏ó‡∏µ‡πà‡πÇ‡∏°‡πÄ‡∏î‡∏•‡∏ó‡∏≥‡∏ô‡∏≤‡∏¢

**‡πÄ‡∏õ‡πâ‡∏≤‡∏´‡∏°‡∏≤‡∏¢:** ‡∏´‡∏≤‡∏Ñ‡πà‡∏≤ \( w \) ‡∏ó‡∏µ‡πà‡∏ó‡∏≥‡πÉ‡∏´‡πâ \( \hat{y} \) ‡πÉ‡∏Å‡∏•‡πâ‡∏Å‡∏±‡∏ö \( y \) ‡∏°‡∏≤‡∏Å‡∏ó‡∏µ‡πà‡∏™‡∏∏‡∏î (‡∏•‡∏î‡∏Ñ‡πà‡∏≤ cost function)

---

## üî∑ 2. ‡∏ß‡∏¥‡∏ò‡∏µ‡∏Å‡∏≤‡∏£‡∏Ñ‡∏≥‡∏ô‡∏ß‡∏ì

### ‡∏ü‡∏±‡∏á‡∏Å‡πå‡∏ä‡∏±‡∏ô Cost:

$$
J(w) = \frac{1}{2m} \sum_{i=1}^{m} (\hat{y}^{(i)} - y^{(i)})^2
$$

‡∏´‡∏≤‡∏Ñ‡πà‡∏≤ \( w \) ‡πÇ‡∏î‡∏¢‡πÅ‡∏Å‡πâ:

$$
\frac{\partial J(w)}{\partial w} = 0
$$

‡∏Ñ‡∏≥‡∏ï‡∏≠‡∏ö‡∏Ñ‡∏∑‡∏≠:

$$
w = (X^T X)^{-1} X^T y
$$

> üìå ‡∏ô‡∏µ‡πà‡∏Ñ‡∏∑‡∏≠ **Normal Equation**

---

## üî∑ 3. ‡∏ï‡∏±‡∏ß‡∏≠‡∏¢‡πà‡∏≤‡∏á

### ‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏£‡∏≤‡∏Ñ‡∏≤‡∏ö‡πâ‡∏≤‡∏ô:

| ‡∏Ç‡∏ô‡∏≤‡∏î‡∏ö‡πâ‡∏≤‡∏ô (‡∏ï‡∏£.‡∏ü‡∏∏‡∏ï) | ‡∏£‡∏≤‡∏Ñ‡∏≤ (‡∏û‡∏±‡∏ô‡∏ö‡∏≤‡∏ó) |
|---------------------|----------------|
| 1000                | 200            |
| 1500                | 300            |
| 2000                | 400            |

### ‡πÄ‡∏ï‡∏£‡∏µ‡∏¢‡∏°‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•:

$$
X = \begin{bmatrix}1 & 1000\\ 1 & 1500\\ 1 & 2000\end{bmatrix}, \quad
y = \begin{bmatrix}200\\ 300\\ 400\end{bmatrix}
$$

### ‡∏Ñ‡∏≥‡∏ô‡∏ß‡∏ì‡∏û‡∏≤‡∏£‡∏≤‡∏°‡∏¥‡πÄ‡∏ï‡∏≠‡∏£‡πå:

$$
w = (X^T X)^{-1} X^T y
$$

---

## üî∑ 4. ‡∏Ç‡πâ‡∏≠‡∏î‡∏µ‡∏Ç‡∏≠‡∏á Normal Equation

- ‚úÖ ‡πÑ‡∏°‡πà‡∏ï‡πâ‡∏≠‡∏á‡πÉ‡∏ä‡πâ‡∏Å‡∏≤‡∏£‡∏ß‡∏ô‡∏•‡∏π‡∏õ  
- ‚úÖ ‡πÑ‡∏î‡πâ‡∏ú‡∏•‡∏•‡∏±‡∏û‡∏ò‡πå‡πÅ‡∏°‡πà‡∏ô‡∏¢‡∏≥  
- ‚úÖ ‡πÄ‡∏Ç‡πâ‡∏≤‡πÉ‡∏à‡∏á‡πà‡∏≤‡∏¢

---

## üî∑ 5. ‡∏Ç‡πâ‡∏≠‡πÄ‡∏™‡∏µ‡∏¢‡∏Ç‡∏≠‡∏á Normal Equation

- ‚ùå ‡∏ä‡πâ‡∏≤‡∏°‡∏≤‡∏Å‡πÄ‡∏°‡∏∑‡πà‡∏≠‡∏ü‡∏µ‡πÄ‡∏à‡∏≠‡∏£‡πå‡∏°‡∏µ‡∏à‡∏≥‡∏ô‡∏ß‡∏ô‡∏°‡∏≤‡∏Å  
- ‚ùå ‡πÉ‡∏ä‡πâ‡∏´‡∏ô‡πà‡∏ß‡∏¢‡∏Ñ‡∏ß‡∏≤‡∏°‡∏à‡∏≥‡∏™‡∏π‡∏á  
- ‚ùå ‡πÉ‡∏ä‡πâ‡πÑ‡∏î‡πâ‡πÄ‡∏â‡∏û‡∏≤‡∏∞‡∏Å‡∏±‡∏ö Linear Regression

---

## üî∑ 6. ‡πÄ‡∏õ‡∏£‡∏µ‡∏¢‡∏ö‡πÄ‡∏ó‡∏µ‡∏¢‡∏ö‡∏Å‡∏±‡∏ö Gradient Descent

| ‡∏´‡∏±‡∏ß‡∏Ç‡πâ‡∏≠              | Normal Equation                | Gradient Descent                      |
|---------------------|--------------------------------|----------------------------------------|
| ‡∏Ñ‡∏ß‡∏≤‡∏°‡πÄ‡∏£‡πá‡∏ß            | ‡πÄ‡∏£‡πá‡∏ß‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏Ç‡∏ô‡∏≤‡∏î‡πÄ‡∏•‡πá‡∏Å       | ‡πÄ‡∏£‡πá‡∏ß‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏Ç‡∏ô‡∏≤‡∏î‡πÉ‡∏´‡∏ç‡πà             |
| ‡∏Ñ‡∏ß‡∏≤‡∏°‡∏ã‡∏±‡∏ö‡∏ã‡πâ‡∏≠‡∏ô         | ‡∏õ‡∏≤‡∏ô‡∏Å‡∏•‡∏≤‡∏á                        | ‡∏ï‡πâ‡∏≠‡∏á‡πÄ‡∏£‡∏µ‡∏¢‡∏ô‡∏£‡∏π‡πâ‡∏≠‡∏ô‡∏∏‡∏û‡∏±‡∏ô‡∏ò‡πå‡πÅ‡∏•‡∏∞‡∏Å‡∏≤‡∏£‡∏õ‡∏£‡∏±‡∏ö‡∏û‡∏≤‡∏£‡∏≤‡∏°‡∏¥‡πÄ‡∏ï‡∏≠‡∏£‡πå |
| ‡∏Ç‡πâ‡∏≠‡∏à‡∏≥‡∏Å‡∏±‡∏î            | ‡πÉ‡∏ä‡πâ‡πÑ‡∏î‡πâ‡πÄ‡∏â‡∏û‡∏≤‡∏∞ Linear Regression | ‡πÉ‡∏ä‡πâ‡πÑ‡∏î‡πâ‡∏Å‡∏±‡∏ö‡πÇ‡∏°‡πÄ‡∏î‡∏•‡∏´‡∏•‡∏≤‡∏Å‡∏´‡∏•‡∏≤‡∏¢               |
| Learning Rate       | ‡πÑ‡∏°‡πà‡∏ï‡πâ‡∏≠‡∏á‡∏õ‡∏£‡∏±‡∏ö                    | ‡∏ï‡πâ‡∏≠‡∏á‡∏õ‡∏£‡∏±‡∏ö                              |

---
