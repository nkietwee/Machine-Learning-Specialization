## Multiple regresion
---

# üîç Overview: Multiple Linear Regression

- In the earlier version of linear regression, we used only **one feature** (e.g., size of the house) to predict the output (e.g., price of the house). but for **Multiple Linear Regression** we use **multiple feature**  such as:
Size of the house (`x‚ÇÅ`), Number of bedrooms (`x‚ÇÇ`), Number of floors (`x‚ÇÉ`) and Age of the house (`x‚ÇÑ`) for predict output value


### üìö Notation
# üìä Multiple Features (Variables)


| Size in feet¬≤ (ùë•‚ÇÅ) | Number of bedrooms (ùë•‚ÇÇ) | Number of floors (ùë•‚ÇÉ) | Age of home in years (ùë•‚ÇÑ) | Price ($1000's) |
|:------------------:|:------------------------:|:----------------------:|:--------------------------:|:----------------:|
|       2104         |            5             |           1            |            45              |       460        |
|       1416         |            3             |           2            |            40              |       232        |
|       1534         |            3             |           2            |            30              |       315        |
|        852         |            2             |           1            |            36              |       178        |
|       ...          |           ...            |          ...           |           ...              |       ...        |


- Let `x‚±º` be the **j·µó ∞ feature** (e.g., `x‚ÇÅ`, `x‚ÇÇ`, ..., `x‚Çô`)
- Let `x‚ÅΩ‚Å±‚Åæ` be the **i·µó ∞ training example** (a vector of features)
- Let `x‚ÅΩ‚Å±‚Åæ‚±º` be the **j·µó ∞ feature** of the i-th training example
- `n` is the **number of features**
- `w = [w‚ÇÅ, w‚ÇÇ, ..., w‚Çô]` is the **weight vector** (parameters)
- `b` is the **bias** (intercept term)
- `x = [x‚ÇÅ, x‚ÇÇ, ..., x‚Çô]` is the **input feature vector**


### üî¢ Notation

- $x_j$ = j·µó ∞ feature  
- $n = 4$ = number of features  
- $\vec{x}^{(i)}$ = features of the i·µó ∞ training example  
- $x_j^{(i)}$ = value of feature j in the i·µó ∞ training example  

### üü¶ Example from the table:

- $i = 2$ ‚Üí second training example  
- $\vec{x}^{(2)} = [1416,\ 3,\ 2,\ 40]$  
- $x_3^{(2)} = 2$



---

# üß† Model Definition

- **Old model (1 feature)**:
  \[
  f_{w,b}(x) = wx + b
  \]

- **New model (n features)**:
  \[
  f_{w,b}(x) = w‚ÇÅx‚ÇÅ + w‚ÇÇx‚ÇÇ + \dots + w‚Çôx‚Çô + b
  \]

- **Compact vector notation using dot product**:
  \[
  f_{w,b}(x) = \vec{w} \cdot \vec{x} + b
  \]


> Dot Product
>\[
>\vec{w} \cdot \vec{x} = \sum_{j=1}^{n} w_j x_j
>\]


# üè† Example Interpretation (House Prices)

\[
f(x) = 0.1x‚ÇÅ + 4x‚ÇÇ + 10x‚ÇÉ - 2x‚ÇÑ + 80
\]

- `0.1`: price increases by $100 per square foot
- `4`: price increases by $4,000 per bedroom
- `10`: price increases by $10,000 per floor
- `-2`: price **decreases** by $2,000 per year of age
- `80`: base price in $1,000s (i.e., $80,000)


# ‚ö° Vectorization 
![alt text](image-1.png)
- **Vectorization** is a technique to make code:
  - **Shorter** (easier to read/write)
  - **Faster** (more computationally efficient)
- Uses **linear algebra operations** and libraries like **NumPy**
- Can take advantage of **parallel hardware** like **CPUs** and **GPUs**

---

## üß† Example Setup

- Suppose we have:
  - `w = [w‚ÇÅ, w‚ÇÇ, w‚ÇÉ]` (weights)
  - `x = [x‚ÇÅ, x‚ÇÇ, x‚ÇÉ]` (features)
  - `b` (bias term)
- We want to compute:
  
  \[
  f(w, b, x) = w_1x_1 + w_2x_2 + w_3x_3 + b
  \]

---

## ‚ùå Non-Vectorized Implementation

### 1. Manual Multiplication (Not Scalable)
```python
f = w[0]*x[0] + w[1]*x[1] + w[2]*x[2] + b
```


### 2. Using a For Loop
```python
f = 0
for j in range(n):  # j = 0 to n-1
    f += w[j] * x[j]
f += b

```
- Better than manual multiplication
- Still not vectorized and slower for large n

![alt text](image-2.png)
![alt text](image-3.png)
# üßæ Terminology

- **Multiple Linear Regression**: Linear regression with **multiple input features**
- **Univariate Regression**: Linear regression with a **single feature**
- Note: **"Multivariate regression"** refers to a different concept (not used here)


## Gradient descent for multiple linear regression
