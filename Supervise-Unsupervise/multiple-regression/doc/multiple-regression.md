## Multiple regresion
---

# üîç Overview: Multiple Linear Regression

- In the earlier version of linear regression, we used only **one feature** (e.g., size of the house) to predict the output (e.g., price of the house). but for **Multiple Linear Regression** we use multiple feature for predict value such as:
  - Size of the house (`x‚ÇÅ`)
  - Number of bedrooms (`x‚ÇÇ`)
  - Number of floors (`x‚ÇÉ`)
  - Age of the house (`x‚ÇÑ`)

![alt text](image.png)

# üìö Notation

- Let `x‚±º` be the **j-th feature** (e.g., `x‚ÇÅ`, `x‚ÇÇ`, ..., `x‚Çô`)
- Let `x‚ÅΩ‚Å±‚Åæ` be the **i-th training example** (a vector of features)
- Let `x‚ÅΩ‚Å±‚Åæ‚±º` be the **j-th feature** of the i-th training example
- `n` is the **number of features**
- `w = [w‚ÇÅ, w‚ÇÇ, ..., w‚Çô]` is the **weight vector** (parameters)
- `b` is the **bias** (intercept term)
- `x = [x‚ÇÅ, x‚ÇÇ, ..., x‚Çô]` is the **input feature vector**

---

# üß† Model Definition

- **Old model (1 feature)**:
  \[
  f_{w,b}(x) = wx + b
  \]

- **New model (n features)**:
  \[
  f_{w,b}(x) = w‚ÇÅx‚ÇÅ + w‚ÇÇx‚ÇÇ + \dots + w‚Çôx‚Çô + b
  \]

- **Compact vector notation using dot product**:
  \[
  f_{w,b}(x) = \vec{w} \cdot \vec{x} + b
  \]


> Dot Product
>\[
>\vec{w} \cdot \vec{x} = \sum_{j=1}^{n} w_j x_j
>\]


# üè† Example Interpretation (House Prices)

\[
f(x) = 0.1x‚ÇÅ + 4x‚ÇÇ + 10x‚ÇÉ - 2x‚ÇÑ + 80
\]

- `0.1`: price increases by $100 per square foot
- `4`: price increases by $4,000 per bedroom
- `10`: price increases by $10,000 per floor
- `-2`: price **decreases** by $2,000 per year of age
- `80`: base price in $1,000s (i.e., $80,000)


# ‚ö° Vectorization 
![alt text](image-1.png)
- **Vectorization** is a technique to make code:
  - **Shorter** (easier to read/write)
  - **Faster** (more computationally efficient)
- Uses **linear algebra operations** and libraries like **NumPy**
- Can take advantage of **parallel hardware** like **CPUs** and **GPUs**

---

## üß† Example Setup

- Suppose we have:
  - `w = [w‚ÇÅ, w‚ÇÇ, w‚ÇÉ]` (weights)
  - `x = [x‚ÇÅ, x‚ÇÇ, x‚ÇÉ]` (features)
  - `b` (bias term)
- We want to compute:
  
  \[
  f(w, b, x) = w_1x_1 + w_2x_2 + w_3x_3 + b
  \]

---

## ‚ùå Non-Vectorized Implementation

### 1. Manual Multiplication (Not Scalable)
```python
f = w[0]*x[0] + w[1]*x[1] + w[2]*x[2] + b
```


### 2. Using a For Loop
```python
f = 0
for j in range(n):  # j = 0 to n-1
    f += w[j] * x[j]
f += b

```
- Better than manual multiplication
- Still not vectorized and slower for large n

![alt text](image-2.png)
![alt text](image-3.png)
# üßæ Terminology

- **Multiple Linear Regression**: Linear regression with **multiple input features**
- **Univariate Regression**: Linear regression with a **single feature**
- Note: **"Multivariate regression"** refers to a different concept (not used here)


## Gradient descent for multiple linear regression
