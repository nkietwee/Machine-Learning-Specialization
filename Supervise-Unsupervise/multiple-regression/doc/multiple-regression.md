## Multiple regresion
---

# ğŸ” Overview: Multiple Linear Regression

- In the earlier version of linear regression, we used only **one feature** (e.g., size of the house) to predict the output (e.g., price of the house). but for **Multiple Linear Regression** we use multiple feature for predict value such as:
  - Size of the house (`xâ‚`)
  - Number of bedrooms (`xâ‚‚`)
  - Number of floors (`xâ‚ƒ`)
  - Age of the house (`xâ‚„`)

![alt text](image.png)

# ğŸ“š Notation

- Let `xâ±¼` be the **j-th feature** (e.g., `xâ‚`, `xâ‚‚`, ..., `xâ‚™`)
- Let `xâ½â±â¾` be the **i-th training example** (a vector of features)
- Let `xâ½â±â¾â±¼` be the **j-th feature** of the i-th training example
- `n` is the **number of features**
- `w = [wâ‚, wâ‚‚, ..., wâ‚™]` is the **weight vector** (parameters)
- `b` is the **bias** (intercept term)
- `x = [xâ‚, xâ‚‚, ..., xâ‚™]` is the **input feature vector**

---

# ğŸ§  Model Definition

- **Old model (1 feature)**:
  \[
  f_{w,b}(x) = wx + b
  \]

- **New model (n features)**:
  \[
  f_{w,b}(x) = wâ‚xâ‚ + wâ‚‚xâ‚‚ + \dots + wâ‚™xâ‚™ + b
  \]

- **Compact vector notation using dot product**:
  \[
  f_{w,b}(x) = \vec{w} \cdot \vec{x} + b
  \]


> Dot Product
>\[
>\vec{w} \cdot \vec{x} = \sum_{j=1}^{n} w_j x_j
>\]


# ğŸ  Example Interpretation (House Prices)

\[
f(x) = 0.1xâ‚ + 4xâ‚‚ + 10xâ‚ƒ - 2xâ‚„ + 80
\]

- `0.1`: price increases by $100 per square foot
- `4`: price increases by $4,000 per bedroom
- `10`: price increases by $10,000 per floor
- `-2`: price **decreases** by $2,000 per year of age
- `80`: base price in $1,000s (i.e., $80,000)


# ğŸ§¾ Terminology

- **Multiple Linear Regression**: Linear regression with **multiple input features**
- **Univariate Regression**: Linear regression with a **single feature**
- Note: **"Multivariate regression"** refers to a different concept (not used here)
